## Handling_Categorical_Features:
One-hot Encoding using.
Python’s category_encoding library.
Scikit-learn preprocessing.
Pandas' get_dummies.

Binary Encoding.
Frequency Encoding.
Label Encoding.
Ordinal Encoding.

## Why do we need encoding?

Most machine learning algorithms cannot handle categorical variables unless we convert them to numerical values
Many algorithm’s performances even vary based upon how the categorical variables are encoded.

## Categorical variables can be divided into two categories:

Nominal: no particular order.
Ordinal: there is some order between values.

## Using Python’s Category Encoder Library

One-hot Encoding.
Label Encoding.
Ordinal Encoding.
Helmert Encoding.
Binary Encoding.
Frequency Encoding.
Mean Encoding.
Weight of Evidence Encoding.
Probability Ratio Encoding.
Hashing Encoding.
Backward Difference Encoding.
Leave One Out Encoding.
James-Stein Encoding.
M-estimator Encoding.
Thermometer Encoder.

## Which Encoding Method is Best?
 

There is no single method that works best for every problem or dataset. I personally think that the get_dummies method has an advantage in its ability to be implemented very easily.

If you want to read about all 15 types of encoding, here is a very good article to refer to.

Here is a cheat sheet on when to use what type of encoding:

![alt text](https://github.com/Sengarofficial/Handling_Categorical_Features/blob/master/garg_cat_variables_15.jpg)


 


